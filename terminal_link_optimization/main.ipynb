{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal Link Optimization\n",
    "\n",
    "This is the main ipython script for the terminal link optimization algorithm.\n",
    "I will update the documentation accordingly once the project has progressed further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make simulation of satellite and customers\n",
    "\n",
    "Lets first start by simulating customers on the ground. This will essentially \n",
    "end up being a straight line with dots (representing customers) being randomly \n",
    "distributed along the line. The line will be spaced according to some \n",
    "arbitrary time metric. Each customer will have (for now) 4 attributes which \n",
    "describe the \"importantness\" of each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets make an array containing the physical location of some simulated customers.\n",
    "The array will have the shape (N_time_segments,length_of_time_segment)\n",
    "\n",
    "customer data array `customer_arr` will be of shape (number of training orbits, number of customers \n",
    "in orbit, number of features describing priority of customers)\n",
    "\"\"\"\n",
    "\n",
    "# training set hyperparameters\n",
    "num_customers = 100 # total number of customers on planet\n",
    "num_orbits = 10000  # total number of training samples (in this case orbits)\n",
    "customer_arr = np.zeros((num_orbits,num_customers,5)) # Define emtpy array to contain customer data\n",
    "\n",
    "for i in range(num_orbits):\n",
    "    # Make customer index labels\n",
    "    customer_arr[i,:,0] = np.arange(start=0,stop=num_customers,step=1)\n",
    "\n",
    "    # Define random locations of customers on a line defining the planet\n",
    "    customer_arr[i,:,1] = np.random.randint(low=0,high=10000,size=(num_customers))\n",
    "    \n",
    "    # We will Define 4 variables which describe the `importantness` of each customer\n",
    "\n",
    "    # Assign a random customer important factor. 0 == low importance, 1 == high importance\n",
    "    customer_arr[i,:,2] = np.random.uniform(low=0.0,high=1.0,size=(num_customers))\n",
    "\n",
    "    # Assign a random customer weather factor. 0 == high cloud coverage, 1 == low cloud coverage\n",
    "    customer_arr[i,:,3] = np.random.uniform(low=0.0,high=1.0,size=(num_customers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose optimal order in which to distribute keys (simple approach)\n",
    "\n",
    "This is an incredibly simple algorithm which only takes into account \n",
    "the importantness and weather factors for each customer. It does not \n",
    "take into account the total time customers have been waiting for \n",
    "a key to be distributed to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop over all available customers within line-of-sight (this is by default set to num_customers)\n",
    "# This is an example using only one training orbit.\n",
    "for i in range(num_customers):\n",
    "    customer_arr[0,i,4] = customer_arr[0,i,2] * customer_arr[0,i,3]\n",
    "\n",
    "customer_prob_list_idx = np.argsort(customer_arr[0,:,4])[::-1]\n",
    "customer_prob_list = customer_arr[0,customer_prob_list_idx,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23., 50., 24., 36., 11., 16., 19., 47., 53., 87., 67., 28., 99.,\n",
       "       58., 71., 83., 21.,  2., 30., 74., 97., 79., 25., 91., 37.,  3.,\n",
       "       64., 65., 45., 44., 40., 13., 42., 15., 98., 60., 55., 72., 35.,\n",
       "        6., 14.,  5., 68., 32., 20., 75., 52., 57., 66., 29., 78., 81.,\n",
       "       86., 17., 51.,  8., 41., 48., 46., 89., 33., 84., 90., 63., 31.,\n",
       "       77., 88., 85., 80., 26.,  9., 62., 27., 73., 34., 43., 96., 10.,\n",
       "       22., 39., 92.,  4.,  0.,  7., 70., 54., 95., 61., 38., 49., 12.,\n",
       "       69., 94., 76.,  1., 93., 59., 82., 18., 56.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out ordered list of customers to distribute keys to\n",
    "customer_prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Approach (complicated approach)\n",
    "\n",
    "I've written some pseudo code down on a piece of paper. Not guranteeing that \n",
    "this will actually work in practice, but I will make my first attempt of this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1920, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This approach uses the Pytorch neural network library\n",
    "\n",
    "# neural network hyperparameters\n",
    "number_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Define network archetecture (convolutional neural network)\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3).double()\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3).double()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(1920, 120).double()\n",
    "        self.fc2 = nn.Linear(120, 84).double()\n",
    "        self.fc3 = nn.Linear(84, 1).double() # one node for predicted probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # Two fully-connected hidden layers and one output layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target, batch_feature_data):\n",
    "    \"\"\" This is a custom loss function which \n",
    "    takes as input the raw output from the neural \n",
    "    network and returns a loss which attempts to \n",
    "    minimize the number of keys not distributed \n",
    "    within the alloted amount of time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def my_model(output, batch_feature_data):\n",
    "        \"\"\" This is a model which will iterate \n",
    "        over the predicted ordering of keys to be \n",
    "        distributed. There will be a penalty \n",
    "        associated with any keys which are predicted to \n",
    "        be distributed outside of the obersvation window.\n",
    "        \"\"\"\n",
    "    \n",
    "        return num_pred_outside_window\n",
    "    \n",
    "    # Get number of keys predicted to be outside of target window\n",
    "    num_pred_outside_window = my_model(output, batch_feature_data)\n",
    "    \n",
    "    loss = torch.mean((num_pred_outside_window - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run network over entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training features: location, importance, weather\n",
    "X_train = customer_arr[:,:,1:4]\n",
    "Y_train = np.zeros((num_orbits,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 17179869184.00 GiB (GPU 0; 1.96 GiB total capacity; 7.17 MiB already allocated; 1.06 GiB free; 974.00 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cad6ea033a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# zero the gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_customers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Made it through iteration!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#        loss = custom_loss(output, Y_train.reshape(batch_size,1).to(device), sampled_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6a653b0b61e6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# If the size is a square you can only specify a single number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_flat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Two fully-connected hidden layers and one output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 17179869184.00 GiB (GPU 0; 1.96 GiB total capacity; 7.17 MiB already allocated; 1.06 GiB free; 974.00 KiB cached)"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "train_split = num_orbits\n",
    "\n",
    "dset_train = TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))\n",
    "dataloader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "# Iterate over entire training set\n",
    "for epoch_num in range(number_epochs):\n",
    "    for i_batch, sampled_batch in enumerate(dataloader):\n",
    "        \n",
    "        # in your training loop:\n",
    "        if (train_split - (batch_size * i_batch)) < batch_size:\n",
    "            break\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(sampled_batch[0].reshape(batch_size,1,num_customers,3).to(device))\n",
    "        print('Made it through iteration!')\n",
    "#        loss = custom_loss(output, Y_train.reshape(batch_size,1).to(device), sampled_batch)\n",
    "#        loss.backward()\n",
    "#        optimizer.step()    # Does the update\n",
    "        \n",
    "    \n",
    "    \n",
    "#    print('Training epoch %d/%d' % (epoch_num+1,number_epochs))\n",
    "    # print statistics\n",
    "#    running_loss += loss.item()\n",
    "#    if i % int(X_train.shape[0]/batch_size) == (int(X_train.shape[0]/batch_size) - 1):    # print every 2000 mini-batches\n",
    "#        print('loss: %.3f' %\n",
    "#                (running_loss / 10))\n",
    "#        running_loss = 0.0\n",
    "#        epoch_num += 1\n",
    "#        print('Training epoch %d/%d' % (epoch_num,number_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
